{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Heart Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File name: 'D6_Heart_Dataset_2.csv'\n",
    "\n",
    "This dataset has been obtained from Kaggle.\n",
    "\n",
    "The dataset contains 303 observations with 13 features and 1 class label with 0 and 1 values.\n",
    "These features are discussed below:\n",
    "1. age: in years\n",
    "2. sex: (1 = male; 0 = female)\n",
    "3. cp: chest pain type (1 = typical angina; 2 = atypical angina; 3 = non-anginal pain; 4 = asymptomatic)\n",
    "4. trestbps: resting blood pressure, in mm Hg on admission to the hospital\n",
    "5. chol: serum cholestrol in mg/dl\n",
    "6. fbs: fasting blood sugar, 120 mg.dl (1 = true; 0 = false)\n",
    "7. restecg: restinng electrocardiographic results (values: 0,1,2)\n",
    "8. thalach: maximum heart ache achieved\n",
    "9. exang: exercise induced angina (1 = yes; 0 = no)\n",
    "10. oldpeak: ST depression induced by exercise relative to rest\n",
    "11. slope: the slope of the peak exercise ST segment\n",
    "12. ca: number of major vessels (0-3) coloured by flouroscopy\n",
    "13. thal: (3 = normal; 6 = fixed defect; 7 = reversable defect)\n",
    "14. target: the predicted attribute, diagnosis of heart disease (0 = fit; 1 = diseased)\n",
    "\n",
    "This is a binary classification problem.\n",
    "Does not contain any categorical data, the dataset is clean. sed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and exploring dataset\n",
    "import pandas as pd\n",
    "#Reading the file into a dataframe\n",
    "#PATH='C:/Users/maria/Dropbox/Machine Learning and Deep Learning/Machine Learning/Undergraduate/Lectures/Datasets and Notebooks' #laptop\n",
    "PATH='C:/Users/admin/Dropbox/Machine Learning and Deep Learning/Machine Learning/Undergraduate/Lectures/Datasets and Notebooks'  #office\n",
    "data=pd.read_csv(f'{PATH}/D6_Heart_Dataset_2.csv')\n",
    "#Displaying the read contents\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating predictors and target\n",
    "X = data.drop(\"target\",axis=1) #predictors\n",
    "Y = data[\"target\"]  #target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and test sets\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X, Y,test_size=0.20,random_state=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - Without Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create logistic regression object\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "logistic_regression1 = LogisticRegression(solver=\"liblinear\", \n",
    "                                         random_state=0) \n",
    "# try different values for max_iter and observe the difference in training time\n",
    "\n",
    "# Train model \n",
    "model1 = logistic_regression1.fit(X_train, Y_train) \n",
    "#Predictions \n",
    "Y_pred1 = model1.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing results\n",
    "from sklearn import metrics \n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.metrics import confusion_matrix, classification_report \n",
    "\n",
    "print(\"The accuracy is \"+str(metrics.accuracy_score(Y_test,Y_pred1)*100)+\"%\") \n",
    "print(confusion_matrix(Y_test, Y_pred1))  \n",
    "target_names = ['class 0', 'class 1'] \n",
    "print(classification_report(Y_test, Y_pred1, target_names=target_names))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - With Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us see values in X_train\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing train data\n",
    "from sklearn import preprocessing\n",
    "standard_scaler = preprocessing.StandardScaler()\n",
    "X_train_standardized=pd.DataFrame(standard_scaler.fit_transform(X_train)) # returns standardized array\n",
    "X_train_standardized\n",
    "\n",
    "# Normalizing train data\n",
    "#normal_scaler = preprocessing.MinMaxScaler()\n",
    "#X_train_normalized=pd.DataFrame(normal_scaler.fit_transform(X_train)) # returns standardized array\n",
    "#X_train_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing test data\n",
    "X_test_standardized=pd.DataFrame(standard_scaler.fit_transform(X_test))\n",
    "X_test_standardized\n",
    "\n",
    "#Normalizing test data\n",
    "#X_test_normalized=pd.DataFrame(normal_scaler.fit_transform(X_test)) # returns standardized array\n",
    "#X_test_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create logistic regression object\n",
    "logistic_regression2 = LogisticRegression(solver=\"liblinear\", \n",
    "                                         random_state=0) \n",
    "# try different values for max_iter and observe the difference in training time\n",
    "\n",
    "# Train model \n",
    "model2 = logistic_regression2.fit(X_train_standardized, Y_train) \n",
    "#Predictions \n",
    "Y_pred2 = model2.predict(X_test_standardized) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Printing results\n",
    "\n",
    "print(\"The accuracy is \"+str(metrics.accuracy_score(Y_test,Y_pred2)*100)+\"%\") \n",
    "print(confusion_matrix(Y_test, Y_pred2))  \n",
    "target_names = ['class 0', 'class 1'] \n",
    "print(classification_report(Y_test, Y_pred2, target_names=target_names)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
